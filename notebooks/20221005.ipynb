{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPapQsFu/8//Jhu1gduraua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyotoman-koshida/bokete/blob/main/notebooks/20221005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### boketeで転移学習したResNet152を使って、MMBTを動かす！"
      ],
      "metadata": {
        "id": "IN3xbvxXiqub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "下準備"
      ],
      "metadata": {
        "id": "0Q2UHRMHi25m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w3hAD2ehZmA",
        "outputId": "f74192af-1696-4b15-c49d-431b2331476b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[ja]\n",
        "!pip install --quiet sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxiIDI1bi7ap",
        "outputId": "3c7a46e4-320d-45a6-98e1-ef38da5042fb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[ja] in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (6.0)\n",
            "Requirement already satisfied: fugashi>=1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.2.0)\n",
            "Requirement already satisfied: unidic>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.1.0)\n",
            "Requirement already satisfied: ipadic<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.0.0)\n",
            "Requirement already satisfied: unidic-lite>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers[ja]) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[ja]) (3.0.9)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2->transformers[ja]) (1.3.5)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2->transformers[ja]) (0.10.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[ja]) (3.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, MMBTForClassification, MMBTConfig, AutoConfig,\n",
        "    Trainer, TrainingArguments,\n",
        ")\n",
        "import transformers\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.models import ResNet152_Weights, resnet152\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "jPFIEJ_Ti-fT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "1WJduRf3jGjI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    img_size = 224\n",
        "    batch_size = 17"
      ],
      "metadata": {
        "id": "w5dWiiIIjIyo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像データをEmbeddingしていきます\n",
        "class ImageEncoder(nn.Module):\n",
        "    POOLING_BREAKDOWN = {1: (1, 1), 2: (2, 1), 3: (3, 1), 4: (2, 2), 5: (5, 1), 6: (3, 2), 7: (7, 1), 8: (4, 2), 9: (3, 3)}\n",
        "    def __init__(self, pretrained_weight):\n",
        "        super().__init__()\n",
        "        model = resnet152(weights=pretrained_weight)\n",
        "        modules = list(model.children())[:-2]\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(self.POOLING_BREAKDOWN[3])\n",
        "\n",
        "    def forward(self,  x):\n",
        "        out = self.pool(self.model(x))\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        return out"
      ],
      "metadata": {
        "id": "76exvNvsk6SE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_jpg(path):\n",
        "    image_tensor = read_image(path)\n",
        "    if image_tensor.shape[0] == 1:\n",
        "        # 1channel=白黒画像があるので3channelにconvertしています。\n",
        "        image_tensor = image_tensor.expand(3, *image_tensor.shape[1:])\n",
        "    return image_tensor\n",
        "\n",
        "class BoketeTextImageDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_seq_len:int, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.image_transforms = image_transform.transforms()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        sentence = torch.tensor(self.tokenizer.encode(row[\"text\"], max_length=self.max_seq_len, padding=\"max_length\", truncation=True))\n",
        "        start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n",
        "        sentence = sentence[:self.max_seq_len]\n",
        "\n",
        "        image = self.image_transforms(read_jpg(row[\"img_path\"]))\n",
        "\n",
        "        return {\n",
        "            \"image_start_token\": start_token,\n",
        "            \"image_end_token\": end_token,\n",
        "            \"sentence\": sentence,\n",
        "            \"image\": image,\n",
        "            \"label\": torch.tensor(row[\"is_laugh\"]),\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    lens = [len(row[\"sentence\"]) for row in batch]\n",
        "    bsz, max_seq_len = len(batch), max(lens)\n",
        "\n",
        "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "\n",
        "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
        "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
        "        mask_tensor[i_batch, :length] = 1\n",
        "\n",
        "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
        "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
        "    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n",
        "    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n",
        "\n",
        "    return {\n",
        "        \"input_ids\":text_tensor,\n",
        "        \"attention_mask\":mask_tensor,\n",
        "        \"input_modal\":img_tensor,\n",
        "        \"modal_start_tokens\":img_start_token,\n",
        "        \"modal_end_tokens\":img_end_token,\n",
        "        \"labels\":tgt_tensor,\n",
        "    }"
      ],
      "metadata": {
        "id": "I1RT0ka3k9Qp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-17iPXxlDD-",
        "outputId": "08d43f97-aa25-4036-ab35-b7d5c3a5a622"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-whole-word-masking/snapshots/ab68bf4a4d55e7772b1fbea6441bdab72aaf949c/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-whole-word-masking/snapshots/ab68bf4a4d55e7772b1fbea6441bdab72aaf949c/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-whole-word-masking/snapshots/ab68bf4a4d55e7772b1fbea6441bdab72aaf949c/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-whole-word-masking/snapshots/ab68bf4a4d55e7772b1fbea6441bdab72aaf949c/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセットの準備"
      ],
      "metadata": {
        "id": "GEQfbdjaj1cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT = \"/content/drive/MyDrive/会社/Nishika/bokete\" # 所望のディレクトリに変更してください。\n",
        "train_image_path = \"/content/drive/MyDrive/会社/Nishika/bokete/content/train/\"\n",
        "test_image_path = \"/content/drive/MyDrive/会社/Nishika/bokete/content/test/\"\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n",
        "submission_df = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n",
        "\n",
        "train_df[\"img_path\"] = train_image_path + train_df[\"odai_photo_file_name\"]\n",
        "test_df[\"img_path\"] = test_image_path + test_df[\"odai_photo_file_name\"]"
      ],
      "metadata": {
        "id": "7ZY5wdcMjJzs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"is_laugh\"] = 0"
      ],
      "metadata": {
        "id": "S6Cq3g_aj8Rt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trn_idx, val_idx = train_test_split(list(range(len(train_df))), test_size=0.2, random_state=42, stratify=train_df[\"is_laugh\"])"
      ],
      "metadata": {
        "id": "cys_DoA6j9xx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MMBTを回す準備"
      ],
      "metadata": {
        "id": "uptIwdG6lmwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trn_ds = BoketeTextImageDataset(train_df.iloc[trn_idx], tokenizer, 48, image_transform=ResNet152_Weights.IMAGENET1K_V2)\n",
        "val_ds = BoketeTextImageDataset(train_df.iloc[val_idx], tokenizer, 48, image_transform=ResNet152_Weights.IMAGENET1K_V2)"
      ],
      "metadata": {
        "id": "-fVhHaAakWd_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = BoketeTextImageDataset(test_df, tokenizer, 48, image_transform=ResNet152_Weights.IMAGENET1K_V2)"
      ],
      "metadata": {
        "id": "6h9gnyk7kXKR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_config = AutoConfig.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
        "transformer = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Naj03zm0lOaz",
        "outputId": "eb215937-6b3f-4d6d-cce9-e281db0c26c4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-whole-word-masking/snapshots/ab68bf4a4d55e7772b1fbea6441bdab72aaf949c/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-whole-word-masking/snapshots/ab68bf4a4d55e7772b1fbea6441bdab72aaf949c/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-whole-word-masking/snapshots/ab68bf4a4d55e7772b1fbea6441bdab72aaf949c/pytorch_model.bin\n",
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = MMBTConfig(transformer_config, num_labels=2)\n",
        "# 転移学習した重みのロードをする\n",
        "model = MMBTForClassification(config, transformer, ImageEncoder(torch.load('/content/drive/MyDrive/会社/Nishika/bokete/bokete-resnet152.pt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOmAdRTvlQ2R",
        "outputId": "e18f8d62-f014-4eb9-9379-cf4335f8644d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.use_return_dict = True"
      ],
      "metadata": {
        "id": "O3ALobXhlaiD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config = model.mmbt.config"
      ],
      "metadata": {
        "id": "YDedKbLblc5F"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/会社/Nishika/bokete/checkpoints\",\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=12,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    gradient_accumulation_steps=20,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEp1dhxclfOk",
        "outputId": "576bfb3b-c8e1-4c1e-cec7-27d908dfcd80"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=trainer_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=trn_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX0EiFO8lq-6",
        "outputId": "5927419c-3eca-4dde-826c-83367fd3c170"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MMBTの学習を行う！"
      ],
      "metadata": {
        "id": "e1BFyl2GlwTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "BZAjRkmjlu2H",
        "outputId": "2ba691f3-7ac5-4475-d21b-2b4ac26d1a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 19969\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 160\n",
            "  Gradient Accumulation steps = 20\n",
            "  Total optimization steps = 372\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 78/372 3:09:11 < 12:11:51, 0.01 it/s, Epoch 0.62/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.683400</td>\n",
              "      <td>0.654249</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/mmbt_exp01/checkpoint-50\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/mmbt_exp01/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in /content/mmbt_exp01/checkpoint-50/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習を終えたMMBTを使った精度検証"
      ],
      "metadata": {
        "id": "jfgYbUu0l5tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds = trainer.predict(val_ds).predictions"
      ],
      "metadata": {
        "id": "uvvahQCzl4Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "log_loss(val_ds.df[\"is_laugh\"].values, softmax(val_preds, axis=-1))"
      ],
      "metadata": {
        "id": "AmRuH1j3l_gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(val_ds.df[\"is_laugh\"].values, np.argmax(val_preds, axis=-1))"
      ],
      "metadata": {
        "id": "N1aAvoJMmB88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "いよいよMMBTの予測を行う！"
      ],
      "metadata": {
        "id": "3PKXOXo1mIs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(test_ds).predictions"
      ],
      "metadata": {
        "id": "uOc_ceNQmMiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df[\"is_laugh\"] = softmax(preds, axis=-1)[:, 1]"
      ],
      "metadata": {
        "id": "lLTeR5DimWyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df[\"is_laugh\"] = submission_df[\"is_laugh\"].astype(float)"
      ],
      "metadata": {
        "id": "3s0tbptamYdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT = \"/content/drive/MyDrive/会社/Nishika/bokete/results\" # ディレクトリを指定してください\n",
        "submission_df.to_csv(os.path.join(OUTPUT,'bokete-ResNet152-bert-mmbt.csv'), index=False)"
      ],
      "metadata": {
        "id": "OgVhxX-tmakd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_3GIGa7Fmd46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "-iunBTZcct0-"
      }
    }
  ]
}